# Support Vector Machine Classifier- binary

![image](https://github.com/kunalpaliwal13/60-Hours-of-ML/assets/143526414/40a2af5b-eee1-46ae-8557-12130cede1bf)

![image](https://github.com/kunalpaliwal13/60-Hours-of-ML/assets/143526414/95ba2f62-0d79-4949-852c-713b5c56afec)

![image](https://github.com/kunalpaliwal13/60-Hours-of-ML/assets/143526414/f38e3a01-4011-406d-b8a9-2cc308080ee2)

![image](https://github.com/kunalpaliwal13/60-Hours-of-ML/assets/143526414/be0b006b-366f-412c-9f45-d9717ce85843)

![image](https://github.com/kunalpaliwal13/60-Hours-of-ML/assets/143526414/4473ab0c-5ed7-4669-a8fb-a474ecea8f4b)

![image](https://github.com/kunalpaliwal13/60-Hours-of-ML/assets/143526414/f4ceb923-abbd-4113-9870-4dacb13a4abf)

![image](https://github.com/kunalpaliwal13/60-Hours-of-ML/assets/143526414/00894323-b8ec-4635-89f6-d14c8e0de569)

Svm multiplies the data point of  a group to the parameters of the line and defines it into different classes, positive and negative. Unlike in logistic regression, where the binary classes are labelled as 0 and 1

![image](https://github.com/kunalpaliwal13/60-Hours-of-ML/assets/143526414/7f442cd2-bc0b-482f-8701-a1533c8d0735)

![image](https://github.com/kunalpaliwal13/60-Hours-of-ML/assets/143526414/b3946e9c-c8b6-400a-8aea-7b590be6c68f)

![image](https://github.com/kunalpaliwal13/60-Hours-of-ML/assets/143526414/2674e661-5380-4776-aaaa-e024a61729f3)

![image](https://github.com/kunalpaliwal13/60-Hours-of-ML/assets/143526414/34dc3242-e516-4bd4-8f99-e1a9a43b3585)

Example of a SVM kernel:

Before:

![image](https://github.com/kunalpaliwal13/60-Hours-of-ML/assets/143526414/e9acb7b9-de74-427a-9e83-a89552c95854)

After:

![image](https://github.com/kunalpaliwal13/60-Hours-of-ML/assets/143526414/8d95b6ae-bb28-4556-81d4-ef0d969ce751)

![image](https://github.com/kunalpaliwal13/60-Hours-of-ML/assets/143526414/323dd914-ea46-4119-83f8-8c0b9597ed27)

![image](https://github.com/kunalpaliwal13/60-Hours-of-ML/assets/143526414/38081ee5-696e-45d2-b889-080b246fe339)

For svm we use a hinge loss instead of the regular loss function as we need to maximimse the margin

![image](https://github.com/kunalpaliwal13/60-Hours-of-ML/assets/143526414/c706d70f-5dc6-489c-9efc-c23f80418992)

Note that wTxi +b is the predicted value.
When we use 1, ie for wrong prediction, the loss value will be high but in correct prediction we use 0, ie loss value will be low

![image](https://github.com/kunalpaliwal13/60-Hours-of-ML/assets/143526414/19b3b7e5-7ab6-49d2-af91-17aa92d9172f)


