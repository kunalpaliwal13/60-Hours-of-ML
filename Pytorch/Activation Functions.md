## Sigmoid Function
- prone to vanishing gradient
- time consuming operations
- suitable for binary classification
- inefficient weight updation(not zero centric)

<image src = "https://github.com/kunalpaliwal13/60-Hours-of-ML/assets/143526414/a6f6914b-2ca5-4717-9a87-2327d3172ccd" height="300px" alt="sigmoid function">


## Tanh Function
- efficient weight updation (zero centric)
- prone to vanishing gradient (for deep deep NN)
- timely complex


<image src = "https://github.com/kunalpaliwal13/60-Hours-of-ML/assets/143526414/0eb7ce66-a1bc-4bd1-9428-3e2ed894495e" height="300px" alt="tanh function">



## Relu Function
- if(relu output =0){weight updates} else {dead neouron}
- not prone to vanishing gradient (good for deep NN) ‚≠ê
- fast calculation
- not zero centric(inefficient weight updation)

<image src = "https://github.com/kunalpaliwal13/60-Hours-of-ML/assets/143526414/b36964c4-37d8-4ee0-abd0-d715b2b2b768" height="300px" alt="relu function">



